{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-recommenders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Zd93eC2Ydvi",
        "outputId": "805562b3-04d2-40ce-b86d-62a42ec2475e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-recommenders\n",
            "  Downloading tensorflow_recommenders-0.7.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow-recommenders) (1.4.0)\n",
            "Requirement already satisfied: tensorflow>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-recommenders) (2.18.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.9.0->tensorflow-recommenders) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=2.9.0->tensorflow-recommenders) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.9.0->tensorflow-recommenders) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.9.0->tensorflow-recommenders) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.9.0->tensorflow-recommenders) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.9.0->tensorflow-recommenders) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.9.0->tensorflow-recommenders) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.9.0->tensorflow-recommenders) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.9.0->tensorflow-recommenders) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.9.0->tensorflow-recommenders) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.9.0->tensorflow-recommenders) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.9.0->tensorflow-recommenders) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow>=2.9.0->tensorflow-recommenders) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.9.0->tensorflow-recommenders) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.9.0->tensorflow-recommenders) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.9.0->tensorflow-recommenders) (0.1.2)\n",
            "Downloading tensorflow_recommenders-0.7.3-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow-recommenders\n",
            "Successfully installed tensorflow-recommenders-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip uninstall -y tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJbUWk9jrdV_",
        "outputId": "20bc2f39-d311-49a0-dafe-58a6723cb102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Successfully uninstalled tensorflow-2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install tensorflow==\"2.15.0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6DFlEatxYUyJ",
        "outputId": "0499575e-cb72-4a91-917b-0db54f186371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.15.0\n",
            "  Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.14.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
            "  Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting numpy<2.0.0,>=1.23.5 (from tensorflow==2.15.0)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.0)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (4.14.0)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.73.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.1)\n",
            "Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.3/475.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, protobuf, numpy, keras, ml-dtypes, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax 0.5.2 requires ml_dtypes>=0.4.0, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.15.0 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.15.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.15.0 which is incompatible.\n",
            "tensorstore 0.1.74 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 numpy-1.26.4 protobuf-4.25.8 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              },
              "id": "2f85de7741514f00a5a418045489d691"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\""
      ],
      "metadata": {
        "id": "W7chVN4M7Dmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aivgAOfpo_Uw",
        "outputId": "2b61556c-408d-48e8-901d-9c44be707cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "# import Google Drive 套件\n",
        "from google.colab import drive\n",
        "# 將自己的雲端硬碟掛載上去\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC_p4z_p3qRe",
        "outputId": "9f39605f-6f61-49c3-b13c-ddbe817e999c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
        "        print('Using GPU')\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "else:\n",
        "    print('No GPU found')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xh4OdKxq2U3C",
        "outputId": "b60147f6-59a8-42e2-bd74-8a7fddcbbe32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "gdrive/My Drive/recommendation/"
      ],
      "metadata": {
        "id": "F_UWBnN5uGH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1. 載入與準備資料\n",
        "# ---------------------------------------------------------------------\n",
        "# --- 載入資料 ---\n",
        "df_project = pd.read_parquet(\"gdrive/My Drive/recommendation/recommend_project.parquet\")\n",
        "df_subscribe = pd.read_csv(\"gdrive/My Drive/recommendation/recommend_subscribe.csv\")\n",
        "df_member = pd.read_csv(\"gdrive/My Drive/recommendation/recommend_member.csv\")\n",
        "df_member_tag = pd.read_csv('gdrive/My Drive/recommendation/recommend_member_tag.csv')\n",
        "df_project_sub_category = pd.read_csv(\"gdrive/My Drive/recommendation/recommend_project_sub_category.csv\")\n",
        "df_review = pd.read_csv('gdrive/My Drive/recommendation/recommend_review.csv')\n",
        "df_follower = pd.read_csv('gdrive/My Drive/recommendation/recommend_follow.csv')\n",
        "df_member_click_6_months = pd.read_csv(\"gdrive/My Drive/recommendation/recommend_click.csv\")\n",
        "\n",
        "\n",
        "# --- member features ---\n",
        "df_member[\"gender\"] = df_member[\"gender\"].fillna(\"Unknown\")\n",
        "df_member[\"gender_encoded\"] = df_member[\"gender\"].astype(\"category\").cat.codes\n",
        "today = pd.Timestamp.today()\n",
        "df_member['birthday'] = pd.to_datetime(df_member['birthday'], errors='coerce')\n",
        "valid_bday = (df_member['birthday'] >= pd.Timestamp('1900-01-01')) & (df_member['birthday'] <= today)\n",
        "df_member.loc[~valid_bday, 'birthday'] = pd.NaT\n",
        "df_member['age'] = (today - df_member['birthday']).dt.days / 365.25\n",
        "df_member['age'] = df_member['age'].fillna(df_member['age'].median())\n",
        "df_member['age_bucket'] = pd.cut(df_member['age'], bins=[0, 18, 25, 35, 50, 100], labels=False, right=False)\n",
        "df_member['age_bucket'] = df_member['age_bucket'].fillna(0).astype(int)\n",
        "user_tags = df_member_tag.groupby('member_id')['type_id'].agg(list).reset_index()\n",
        "df_member = df_member.merge(user_tags, on='member_id', how='left')\n",
        "df_member.rename(columns={'type_id': 'user_tag_ids'}, inplace=True)\n",
        "df_member['user_tag_ids'] = df_member['user_tag_ids'].apply(lambda x: x if isinstance(x, list) else ['0'])\n",
        "df_member['user_tag_ids'] = df_member['user_tag_ids'].apply(lambda l: [str(x) for x in l])\n",
        "count = df_member['user_tag_ids'].apply(lambda x: x == ['0']).sum()\n",
        "count_before = df_member['user_tag_ids'].apply(lambda x: x == ['0']).sum()\n",
        "print(f\"修改前，有 {count_before} 位用戶的 user_tag_ids 是 ['0']\")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# --- 從用戶購買歷史推斷隱含興趣標籤 ---\n",
        "# ---------------------------------------------------------------------\n",
        "print(\"\\n--- 開始從購買歷史推斷用戶興趣標籤 ---\")\n",
        "item_sub_cate = df_project_sub_category.groupby('project_id')['sub_category'].agg(list).reset_index()\n",
        "df_project = df_project.merge(item_sub_cate, on='project_id', how='left')\n",
        "df_project.rename(columns={'sub_category': 'sub_categories'}, inplace=True)\n",
        "df_project['sub_categories'] = df_project['sub_categories'].apply(lambda x: x if isinstance(x, list) else [])\n",
        "\n",
        "df_project_for_tags = df_project[['project_id', 'sub_categories']]\n",
        "df_subscribe_with_cate = pd.merge(df_subscribe, df_project_for_tags, on='project_id', how='left')\n",
        "\n",
        "def flatten_and_unique(series_of_lists):\n",
        "    all_items = [item for sublist in series_of_lists.dropna() for item in sublist]\n",
        "    return list(set(all_items))\n",
        "\n",
        "inferred_tags = df_subscribe_with_cate.groupby('member_id')['sub_categories'].apply(flatten_and_unique).reset_index()\n",
        "inferred_tags.rename(columns={'sub_categories': 'inferred_user_tags'}, inplace=True)\n",
        "\n",
        "df_member = pd.merge(df_member, inferred_tags, on='member_id', how='left')\n",
        "df_member['inferred_user_tags'] = df_member['inferred_user_tags'].apply(lambda x: x if isinstance(x, list) else [])\n",
        "\n",
        "def update_tags(row):\n",
        "    is_default_tag = (row['user_tag_ids'] == ['0'])\n",
        "    has_inferred_tags = bool(row['inferred_user_tags'])\n",
        "\n",
        "    if is_default_tag:\n",
        "        if has_inferred_tags:\n",
        "            return row['inferred_user_tags']\n",
        "        else:\n",
        "            return row['user_tag_ids']\n",
        "    else:\n",
        "        if has_inferred_tags:\n",
        "            return list(set(row['inferred_user_tags'] + row['user_tag_ids']))\n",
        "        else:\n",
        "            return row['user_tag_ids']\n",
        "\n",
        "df_member['user_tag_ids'] = df_member.apply(update_tags, axis=1)\n",
        "df_member['user_tag_ids'] = df_member['user_tag_ids'].apply(lambda l: [str(x) for x in l])\n",
        "count_after = df_member['user_tag_ids'].apply(lambda x: x == ['0']).sum()\n",
        "print(f\"修改後，剩下 {count_after} 位用戶的 user_tag_ids 是 ['0']\")\n",
        "print(\"--- 用戶興趣標籤推斷完成 ---\")\n",
        "\n",
        "# --- project features ---\n",
        "df_project = pd.read_parquet(\"gdrive/My Drive/recommendation/recommend_project.parquet\")\n",
        "embedding_matrix = df_project['content_embedding'].apply(pd.Series).values\n",
        "# Apply PCA to reduce dimensions to 768\n",
        "pca = PCA(n_components=256)\n",
        "reduced_embeddings = pca.fit_transform(embedding_matrix)\n",
        "\n",
        "# Add reduced embeddings back to the DataFrame if needed\n",
        "df_project['content_embedding'] = reduced_embeddings.tolist()\n",
        "df_project['content_embedding'] = df_project['content_embedding'].apply(lambda x: np.asarray(x, dtype=np.float32))\n",
        "item_sub_cate = df_project_sub_category.groupby('project_id')['sub_category'].agg(list).reset_index()\n",
        "df_project = df_project.merge(item_sub_cate, on='project_id', how='left')\n",
        "df_project.rename(columns={'sub_category': 'sub_categories'}, inplace=True)\n",
        "df_project['sub_categories'] = df_project['sub_categories'].apply(lambda x: x if isinstance(x, list) else [])\n",
        "df_project['start_date'] = pd.to_datetime(df_project['start_date'])\n",
        "df_project['launch_age'] = (today - df_project['start_date']).dt.days\n",
        "df_project['launch_age'] = df_project['launch_age'].fillna(df_project['launch_age'].median())\n",
        "review_count = df_review.groupby('project_id').size().rename('num_reviews')\n",
        "follower_count = df_follower.groupby('project_id').size().rename('total_followers')\n",
        "df_project = df_project.merge(review_count, on='project_id', how='left')\n",
        "df_project = df_project.merge(follower_count, on='project_id', how='left')\n",
        "stat_cols = ['purchase_cnt', 'rating', 'num_reviews', 'total_followers','launch_age']\n",
        "df_project[stat_cols] = df_project[stat_cols].fillna(0)\n",
        "for col in stat_cols:\n",
        "    df_project[col] = pd.to_numeric(df_project[col], errors='coerce')\n",
        "    df_project[col] = df_project[col].fillna(0)\n",
        "    df_project['log_' + col] = np.log1p(df_project[col])\n",
        "df_project = df_project[[\"project_id\", \"content_embedding\",\"main_category\",\"log_launch_age\",\"log_purchase_cnt\",\"log_rating\",\"log_num_reviews\",\"log_total_followers\",\"sub_categories\"]]\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2. 處理點擊歷史與冷啟動特徵\n",
        "# ---------------------------------------------------------------------\n",
        "print(\"\\n--- 處理點擊歷史與冷啟動特徵 ---\")\n",
        "\n",
        "df_member_click_6_months['record_date'] = pd.to_datetime(df_member_click_6_months['record_date'])\n",
        "df_subscribe['subscribe_date'] = pd.to_datetime(df_subscribe['subscribe_date'])\n",
        "\n",
        "purchased_projects = df_subscribe.groupby('member_id')['project_id'].apply(set).to_dict()\n",
        "def filter_purchased_clicks(row):\n",
        "    member_id = row['member_id']\n",
        "    project_id = row['project_id']\n",
        "    if member_id in purchased_projects:\n",
        "        return project_id not in purchased_projects[member_id]\n",
        "    return True\n",
        "df_click_filtered = df_member_click_6_months[df_member_click_6_months.apply(filter_purchased_clicks, axis=1)].copy()\n",
        "\n",
        "\n",
        "df_click_filtered = df_click_filtered.sort_values(['member_id', 'record_date'])\n",
        "MAX_SEQ_LENGTH = 50\n",
        "user_click_sequences = df_click_filtered.groupby('member_id').apply(\n",
        "    lambda g: g.sort_values('record_date').tail(MAX_SEQ_LENGTH)['project_id'].tolist()\n",
        ").to_dict()\n",
        "\n",
        "\n",
        "df_member['click_sequence'] = df_member['member_id'].map(user_click_sequences)\n",
        "df_member['click_sequence'] = df_member['click_sequence'].apply(lambda x: x if isinstance(x, list) else [])\n",
        "\n",
        "df_member['num_clicks'] = df_member['click_sequence'].apply(len)\n",
        "df_member['is_cold_user'] = (df_member['num_clicks'] == 0).astype(np.float32)\n",
        "\n",
        "print(f\"總用戶數: {len(df_member)}\")\n",
        "print(f\"平均點擊次數: {df_member['num_clicks'].mean():.2f}\")\n",
        "print(f\"冷啟動用戶數 (is_cold_user=1): {(df_member['is_cold_user'] == 1).sum()}\")\n",
        "print(f\"暖啟動用戶數 (is_cold_user=0): {(df_member['is_cold_user'] == 0).sum()}\")\n",
        "\n",
        "\n",
        "# --- 建立互動資料表 ---\n",
        "df_subscribe = pd.merge(df_member, df_subscribe, on='member_id', how='right')\n",
        "df_subscribe = pd.merge(df_project, df_subscribe, on='project_id')\n",
        "\n",
        "df_subscribe = df_subscribe[[\n",
        "    'member_id', 'project_id', 'gender_encoded', 'main_category', 'content_embedding',\n",
        "    'age_bucket', 'sub_categories', 'user_tag_ids', 'click_sequence',\n",
        "    'is_cold_user',\n",
        "    'log_launch_age', 'log_purchase_cnt', 'log_rating', 'log_num_reviews', 'log_total_followers',\n",
        "    'subscribe_date'\n",
        "]]\n",
        "\n",
        "df_subscribe.drop_duplicates(subset=[\"member_id\", \"project_id\"], inplace=True)\n",
        "df_subscribe = df_subscribe.sort_values(\"subscribe_date\").reset_index(drop=True)\n",
        "\n",
        "print(f\"最終互動資料表 shape: {df_subscribe.shape}\")\n",
        "print(f\"互動資料表欄位: {df_subscribe.columns.tolist()}\")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3. 建立 tf.data 資料集\n",
        "# ---------------------------------------------------------------------\n",
        "def pad_sequence(seq, max_length=MAX_SEQ_LENGTH, pad_value=\"0\"):\n",
        "    if len(seq) >= max_length:\n",
        "        return seq[-max_length:]\n",
        "    else:\n",
        "        return [pad_value] * (max_length - len(seq)) + seq\n",
        "\n",
        "padded_click_sequences = [pad_sequence(seq) for seq in df_subscribe[\"click_sequence\"]]\n",
        "\n",
        "interactions_dict = {\n",
        "    \"member_id\"        : df_subscribe[\"member_id\"].astype(str).values,\n",
        "    \"gender_encoded\"   : df_subscribe[\"gender_encoded\"].astype(str).values,\n",
        "    \"project_id\"       : df_subscribe[\"project_id\"].astype(str).values,\n",
        "    \"main_category\"    : df_subscribe[\"main_category\"].astype(str).values,\n",
        "    \"content_embedding\": np.stack(df_subscribe[\"content_embedding\"].values),\n",
        "    \"age_bucket\"       : df_subscribe[\"age_bucket\"].astype(str).values,\n",
        "    \"sub_categories\"   : tf.ragged.constant(df_subscribe[\"sub_categories\"].to_list()),\n",
        "    \"user_tag_ids\"     : tf.ragged.constant(df_subscribe[\"user_tag_ids\"].to_list()),\n",
        "    \"click_sequence\"   : tf.constant(padded_click_sequences, dtype=tf.string),\n",
        "    \"is_cold_user\"     : df_subscribe[\"is_cold_user\"].values,\n",
        "    \"log_launch_age\"   : df_subscribe[\"log_launch_age\"].astype(float).values,\n",
        "    \"log_purchase_cnt\" : df_subscribe[\"log_purchase_cnt\"].astype(float).values,\n",
        "    \"log_rating\"       : df_subscribe[\"log_rating\"].astype(float).values,\n",
        "    \"log_num_reviews\"  : df_subscribe[\"log_num_reviews\"].astype(float).values,\n",
        "    \"log_total_followers\": df_subscribe[\"log_total_followers\"].astype(float).values,\n",
        "}\n",
        "\n",
        "interactions = tf.data.Dataset.from_tensor_slices(interactions_dict)\n",
        "\n",
        "# Project 資料集\n",
        "projects_dict = {\n",
        "    \"project_id\"       : df_project[\"project_id\"].astype(str).values,\n",
        "    \"main_category\"    : df_project[\"main_category\"].astype(str).values,\n",
        "    \"content_embedding\": np.stack(df_project[\"content_embedding\"].values),\n",
        "    \"sub_categories\": tf.ragged.constant(df_project[\"sub_categories\"].to_list()),\n",
        "    \"log_launch_age\"   : df_project[\"log_launch_age\"].astype(float).values,\n",
        "    \"log_purchase_cnt\" : df_project[\"log_purchase_cnt\"].astype(float).values,\n",
        "    \"log_rating\"       : df_project[\"log_rating\"].astype(float).values,\n",
        "    \"log_num_reviews\"  : df_project[\"log_num_reviews\"].astype(float).values,\n",
        "    \"log_total_followers\": df_project[\"log_total_followers\"].astype(float).values,\n",
        "}\n",
        "projects = tf.data.Dataset.from_tensor_slices(projects_dict)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4. 建立詞彙表\n",
        "# ---------------------------------------------------------------------\n",
        "unique_member_ids = np.unique(np.concatenate(list(interactions.batch(1_000).map(lambda x: x[\"member_id\"]))))\n",
        "unique_genders = np.unique(np.concatenate(list(interactions.batch(1_000).map(lambda x: x[\"gender_encoded\"]))))\n",
        "unique_project_ids = np.unique(np.concatenate(list(projects.batch(1_000).map(lambda x: x[\"project_id\"]))))\n",
        "unique_main_categories = np.unique(np.concatenate(list(projects.batch(1_000).map(lambda x: x[\"main_category\"]))))\n",
        "unique_age_buckets = np.unique(np.concatenate(list(interactions.batch(1_000).map(lambda x: x[\"age_bucket\"]))))\n",
        "\n",
        "all_click_projects = set()\n",
        "for seq in df_subscribe[\"click_sequence\"]:\n",
        "    all_click_projects.update(seq)\n",
        "project_ids_as_strings = [id.decode('utf-8') if isinstance(id, bytes) else str(id) for id in unique_project_ids.tolist()]\n",
        "all_click_projects.update(project_ids_as_strings)\n",
        "all_click_projects.discard('')\n",
        "unique_sequential_project_ids = sorted([str(id) for id in all_click_projects])\n",
        "\n",
        "ragged_sub_categories = tf.ragged.constant(df_project[\"sub_categories\"].to_list())\n",
        "flat_sub_cats = ragged_sub_categories.flat_values\n",
        "unique_sub_categories = np.unique(flat_sub_cats.numpy())\n",
        "ragged_user_tag_ids = tf.ragged.constant(df_subscribe[\"user_tag_ids\"].to_list())\n",
        "flat_user_tag_ids = ragged_user_tag_ids.flat_values\n",
        "unique_user_tag_ids = np.unique(flat_user_tag_ids.numpy())\n",
        "\n",
        "EMBEDDING_DIM = 32\n",
        "SEQ_EMBEDDING_DIM = 64\n",
        "NUMERIC_FEATURES = [\"log_launch_age\", \"log_purchase_cnt\", \"log_rating\", \"log_num_reviews\", \"log_total_followers\"]\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5. SASRec Encoder\n",
        "# ---------------------------------------------------------------------\n",
        "class SASRecEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, max_seq_length,\n",
        "                 num_heads=4, num_blocks=2, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.num_heads = num_heads\n",
        "        self.num_blocks = num_blocks\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.item_lookup = tf.keras.layers.StringLookup(\n",
        "            vocabulary=unique_sequential_project_ids, mask_token=\"0\", name=\"sasrec_item_lookup\")\n",
        "        self.item_embedding = tf.keras.layers.Embedding(\n",
        "            self.item_lookup.vocabulary_size(), embedding_dim, mask_zero=True)\n",
        "        self.positional_embedding = tf.keras.layers.Embedding(\n",
        "            max_seq_length, embedding_dim)\n",
        "\n",
        "        self.attention_blocks = []\n",
        "        self.feed_forward_blocks = []\n",
        "        self.layer_norms_1 = []\n",
        "        self.layer_norms_2 = []\n",
        "        self.dropouts = []\n",
        "\n",
        "        for _ in range(num_blocks):\n",
        "            self.attention_blocks.append(\n",
        "                MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim//num_heads)\n",
        "            )\n",
        "            self.feed_forward_blocks.append(\n",
        "                tf.keras.Sequential([\n",
        "                    tf.keras.layers.Dense(embedding_dim * 4, activation='relu'),\n",
        "                    tf.keras.layers.Dense(embedding_dim)\n",
        "                ])\n",
        "            )\n",
        "            self.layer_norms_1.append(LayerNormalization())\n",
        "            self.layer_norms_2.append(LayerNormalization())\n",
        "            self.dropouts.append(tf.keras.layers.Dropout(dropout_rate))\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        seq_length = tf.shape(inputs)[1]\n",
        "        item_ids = self.item_lookup(inputs)\n",
        "        item_emb = self.item_embedding(item_ids)\n",
        "        positions = tf.range(seq_length)\n",
        "        pos_emb = self.positional_embedding(positions)\n",
        "        pos_emb = tf.expand_dims(pos_emb, 0)\n",
        "        sequence_emb = item_emb + pos_emb\n",
        "\n",
        "        causal_mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
        "        causal_mask = tf.expand_dims(causal_mask, 0)\n",
        "\n",
        "        for i in range(self.num_blocks):\n",
        "            attn_output = self.attention_blocks[i](\n",
        "                sequence_emb, sequence_emb, attention_mask=causal_mask, training=training)\n",
        "            attn_output = self.dropouts[i](attn_output, training=training)\n",
        "            sequence_emb = self.layer_norms_1[i](sequence_emb + attn_output)\n",
        "\n",
        "            ff_output = self.feed_forward_blocks[i](sequence_emb)\n",
        "            ff_output = self.dropouts[i](ff_output, training=training)\n",
        "            sequence_emb = self.layer_norms_2[i](sequence_emb + ff_output)\n",
        "\n",
        "        mask = self.item_embedding.compute_mask(item_ids)\n",
        "        if mask is not None:\n",
        "            mask = tf.cast(mask, tf.float32)\n",
        "            mask = tf.expand_dims(mask, -1)\n",
        "            sequence_emb = sequence_emb * mask\n",
        "            pooled = tf.reduce_sum(sequence_emb, axis=1) / (tf.reduce_sum(mask, axis=1) + 1e-9)\n",
        "        else:\n",
        "            pooled = tf.reduce_mean(sequence_emb, axis=1)\n",
        "\n",
        "        return pooled\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6. User Model (動靜態雙模式與門控)\n",
        "# ---------------------------------------------------------------------\n",
        "class StaticUserModel(tf.keras.Model):\n",
        "    \"\"\"僅處理靜態特徵的用戶模型，用於冷啟動用戶。\"\"\"\n",
        "    def __init__(self, dynamic_model_output_dim):\n",
        "        super().__init__()\n",
        "        self.user_emb = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(vocabulary=unique_member_ids, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(unique_member_ids) + 1, EMBEDDING_DIM)])\n",
        "        self.gender_emb = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(vocabulary=unique_genders, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(unique_genders) + 1, EMBEDDING_DIM)])\n",
        "        self.age_bucket_emb = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(vocabulary=unique_age_buckets, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(unique_age_buckets) + 1, EMBEDDING_DIM)])\n",
        "        self.user_tag_ids_emb = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(vocabulary=unique_user_tag_ids, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(unique_user_tag_ids) + 1, EMBEDDING_DIM),\n",
        "            tf.keras.layers.GlobalAveragePooling1D()])\n",
        "        # 投影層：將靜態特徵的維度投影到與動態模型輸出維度一致\n",
        "        self.projection = tf.keras.layers.Dense(dynamic_model_output_dim, activation='relu', name='static_projection')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        static_features = tf.concat([\n",
        "            self.user_emb(inputs[\"member_id\"]),\n",
        "            self.gender_emb(inputs[\"gender_encoded\"]),\n",
        "            self.age_bucket_emb(inputs[\"age_bucket\"]),\n",
        "            self.user_tag_ids_emb(inputs[\"user_tag_ids\"]),\n",
        "        ], axis=1)\n",
        "        return self.projection(static_features)\n",
        "\n",
        "class DynamicUserModel(tf.keras.Model):\n",
        "    \"\"\"處理靜態+序列特徵的用戶模型，用於暖啟動用戶。\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 靜態特徵層\n",
        "        self.user_emb = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(vocabulary=unique_member_ids, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(unique_member_ids) + 1, EMBEDDING_DIM)])\n",
        "        self.gender_emb = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(vocabulary=unique_genders, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(unique_genders) + 1, EMBEDDING_DIM)])\n",
        "        self.age_bucket_emb = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(vocabulary=unique_age_buckets, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(unique_age_buckets) + 1, EMBEDDING_DIM)])\n",
        "        self.user_tag_ids_emb = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(vocabulary=unique_user_tag_ids, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(unique_user_tag_ids) + 1, EMBEDDING_DIM),\n",
        "            tf.keras.layers.GlobalAveragePooling1D()])\n",
        "        # 序列特徵層\n",
        "        self.sequential_encoder = SASRecEncoder(\n",
        "            vocab_size=len(unique_sequential_project_ids),\n",
        "            embedding_dim=SEQ_EMBEDDING_DIM,\n",
        "            max_seq_length=MAX_SEQ_LENGTH)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        static_features = tf.concat([\n",
        "            self.user_emb(inputs[\"member_id\"]),\n",
        "            self.gender_emb(inputs[\"gender_encoded\"]),\n",
        "            self.age_bucket_emb(inputs[\"age_bucket\"]),\n",
        "            self.user_tag_ids_emb(inputs[\"user_tag_ids\"]),\n",
        "        ], axis=1)\n",
        "        sequential_features = self.sequential_encoder(inputs[\"click_sequence\"])\n",
        "        return tf.concat([static_features, sequential_features], axis=1)\n",
        "\n",
        "    @property\n",
        "    def output_dim(self):\n",
        "        return (4 * EMBEDDING_DIM) + SEQ_EMBEDDING_DIM\n",
        "\n",
        "class GatedUserModel(tf.keras.Model):\n",
        "    \"\"\"門控用戶模型，根據 is_cold_user 在靜態和動態模型間路由。\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dynamic_model = DynamicUserModel()\n",
        "        self.static_model = StaticUserModel(self.dynamic_model.output_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        is_cold_mask = tf.cast(inputs[\"is_cold_user\"], tf.bool)\n",
        "\n",
        "        # BUG FIX: Reshape to (batch_size, 1) to enable broadcasting for tf.where\n",
        "        is_cold_mask_reshaped = tf.reshape(is_cold_mask, [-1, 1])\n",
        "\n",
        "        static_embedding = self.static_model(inputs)\n",
        "        dynamic_embedding = self.dynamic_model(inputs)\n",
        "\n",
        "        # 核心路由邏輯：\n",
        "        # 當 is_cold_mask_reshaped 的元素為 True 時，選用 static_embedding\n",
        "        # 反之，則選用 dynamic_embedding\n",
        "        final_embedding = tf.where(\n",
        "            is_cold_mask_reshaped,\n",
        "            x=static_embedding,\n",
        "            y=dynamic_embedding\n",
        "        )\n",
        "        return final_embedding\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7. Project Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ProjectModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.id_emb = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(vocabulary=unique_project_ids, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(unique_project_ids) + 1, EMBEDDING_DIM),\n",
        "        ])\n",
        "        self.main_category_emb = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(vocabulary=unique_main_categories, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(unique_main_categories) + 1,EMBEDDING_DIM),\n",
        "        ])\n",
        "        self.content_proj = tf.keras.layers.Dense(EMBEDDING_DIM, use_bias=False)\n",
        "        self.sub_category_embedding = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(vocabulary=unique_sub_categories, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(unique_sub_categories) + 1, EMBEDDING_DIM),\n",
        "            tf.keras.layers.GlobalAveragePooling1D()\n",
        "        ])\n",
        "        self.numeric_proj = tf.keras.layers.Dense(\n",
        "            EMBEDDING_DIM, activation=\"relu\"\n",
        "        )\n",
        "    def call(self, inputs):\n",
        "        numeric = tf.stack(\n",
        "            [tf.cast(inputs[f], tf.float32) for f in NUMERIC_FEATURES],\n",
        "            axis=-1\n",
        "        )\n",
        "        return tf.concat([\n",
        "            self.id_emb(inputs[\"project_id\"]),\n",
        "            self.main_category_emb(inputs[\"main_category\"]),\n",
        "            self.content_proj(inputs[\"content_embedding\"]),\n",
        "            self.sub_category_embedding(inputs[\"sub_categories\"]),\n",
        "            self.numeric_proj(numeric),\n",
        "        ], axis=1)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 8. 雙塔模型\n",
        "# ---------------------------------------------------------------------\n",
        "class SequentialTwoTower(tfrs.models.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        l2_regularizer = tf.keras.regularizers.l2(1e-5)\n",
        "\n",
        "        self.query_model = tf.keras.Sequential([\n",
        "            GatedUserModel(),\n",
        "            tf.keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=l2_regularizer),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=l2_regularizer),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.Dense(EMBEDDING_DIM)\n",
        "        ])\n",
        "\n",
        "        self.candidate_model = tf.keras.Sequential([\n",
        "            ProjectModel(),\n",
        "            tf.keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=l2_regularizer),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=l2_regularizer),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.Dense(EMBEDDING_DIM)\n",
        "        ])\n",
        "\n",
        "        self.task = tfrs.tasks.Retrieval(\n",
        "            metrics=tfrs.metrics.FactorizedTopK(\n",
        "                candidates=projects.batch(128).map(self.candidate_model),\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def compute_loss(self, features, training=False):\n",
        "        query_emb = self.query_model({\n",
        "            \"member_id\"      : features[\"member_id\"],\n",
        "            \"gender_encoded\" : features[\"gender_encoded\"],\n",
        "            \"age_bucket\"     : features[\"age_bucket\"],\n",
        "            \"user_tag_ids\"   : features[\"user_tag_ids\"],\n",
        "            \"click_sequence\" : features[\"click_sequence\"],\n",
        "            \"is_cold_user\"   : features[\"is_cold_user\"],\n",
        "        })\n",
        "\n",
        "        candidate_emb = self.candidate_model({\n",
        "            \"project_id\"       : features[\"project_id\"],\n",
        "            \"main_category\"    : features[\"main_category\"],\n",
        "            \"content_embedding\": features[\"content_embedding\"],\n",
        "            \"sub_categories\"   : features[\"sub_categories\"],\n",
        "            \"log_launch_age\"    : features[\"log_launch_age\"],\n",
        "            \"log_purchase_cnt\"    : features[\"log_purchase_cnt\"],\n",
        "            \"log_rating\"          : features[\"log_rating\"],\n",
        "            \"log_num_reviews\"     : features[\"log_num_reviews\"],\n",
        "            \"log_total_followers\" : features[\"log_total_followers\"],\n",
        "        })\n",
        "        return self.task(query_emb, candidate_emb, compute_metrics=not training)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 9. 訓練與評估\n",
        "# ---------------------------------------------------------------------\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "dataset_size = len(df_subscribe)\n",
        "train_size   = int(dataset_size * 0.8)\n",
        "val_size     = int(dataset_size * 0.1)\n",
        "\n",
        "train_ds      = interactions.take(train_size)\n",
        "validation_ds = interactions.skip(train_size).take(val_size)\n",
        "test_ds       = interactions.skip(train_size + val_size)\n",
        "\n",
        "cached_train = (\n",
        "    train_ds.shuffle(train_size, seed=42)\n",
        "            .batch(2048)\n",
        "            .cache()\n",
        "            .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "cached_validation  = (\n",
        "    validation_ds.batch(2048)\n",
        "                 .cache()\n",
        "                 .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "cached_test = (\n",
        "    test_ds.batch(2048)\n",
        "           .cache()\n",
        "           .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "\n",
        "model = SequentialTwoTower()\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_factorized_top_k/top_100_categorical_accuracy',\n",
        "    mode='max',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3\n",
        ")\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.005))\n",
        "epochs = 50\n",
        "\n",
        "print(\"\\n--- Training ---\")\n",
        "history = model.fit(\n",
        "    cached_train,\n",
        "    epochs=epochs,\n",
        "    validation_data=cached_validation,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n--- Evaluation ---\")\n",
        "train_metrics = model.evaluate(cached_train, return_dict=True)\n",
        "validation_metrics = model.evaluate(cached_validation, return_dict=True)\n",
        "test_metrics = model.evaluate(cached_test, return_dict=True)\n",
        "\n",
        "print(f\"\\nTop-100 Accuracy (Train):      {train_metrics['factorized_top_k/top_100_categorical_accuracy']:.4f}\")\n",
        "print(f\"Top-100 Accuracy (Validation): {validation_metrics['factorized_top_k/top_100_categorical_accuracy']:.4f}\")\n",
        "print(f\"Top-100 Accuracy (Test):       {test_metrics['factorized_top_k/top_100_categorical_accuracy']:.4f}\")\n",
        "\n",
        "print(\"\\n--- Retrieval Model Saved ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGwG7A6WqeOg",
        "outputId": "6060d4ec-2ab6-4f08-b17e-6d02868467e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "修改前，有 939612 位用戶的 user_tag_ids 是 ['0']\n",
            "\n",
            "--- 開始從購買歷史推斷用戶興趣標籤 ---\n",
            "修改後，剩下 774492 位用戶的 user_tag_ids 是 ['0']\n",
            "--- 用戶興趣標籤推斷完成 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 處理點擊歷史與冷啟動特徵 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-7-2168005965.py:136: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  user_click_sequences = df_click_filtered.groupby('member_id').apply(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "總用戶數: 1363150\n",
            "平均點擊次數: 0.35\n",
            "冷啟動用戶數 (is_cold_user=1): 1249239\n",
            "暖啟動用戶數 (is_cold_user=0): 113911\n",
            "最終互動資料表 shape: (704377, 16)\n",
            "互動資料表欄位: ['member_id', 'project_id', 'gender_encoded', 'main_category', 'content_embedding', 'age_bucket', 'sub_categories', 'user_tag_ids', 'click_sequence', 'is_cold_user', 'log_launch_age', 'log_purchase_cnt', 'log_rating', 'log_num_reviews', 'log_total_followers', 'subscribe_date']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EM87D_-Kqrk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9z9VHqwPqrpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TxxmR7ABqrtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u4oE2wQPAHOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EdJ2RO4eAHQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AxbyFmkcAHSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5HjhipX1AHVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C0Z-hNRfAHXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qSiZpsiMqGkg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}